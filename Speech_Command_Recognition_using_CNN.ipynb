{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -U tensorflow tensorflow_datasets librosa seaborn\n",
        "\n",
        "# Import necessary modules\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa.display\n",
        "from tensorflow.keras import layers, models\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Ensure the dataset path is correct\n",
        "DATASET_PATH = pathlib.Path(\"data/mini_speech_commands\")\n",
        "\n",
        "# Check and download dataset if not available\n",
        "if not DATASET_PATH.exists():\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        \"mini_speech_commands.zip\",\n",
        "        origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "        extract=True,\n",
        "        cache_dir=\".\",  # Extract inside ./data/\n",
        "        cache_subdir=\"data\"\n",
        "    )\n",
        "\n",
        "# Set correct extracted dataset directory\n",
        "data_dir = pathlib.Path(\"data/mini_speech_commands\")\n",
        "\n",
        "# Confirm dataset extraction\n",
        "if not data_dir.exists():\n",
        "    raise FileNotFoundError(\"Dataset extraction failed. Please check manually.\")\n",
        "else:\n",
        "    print(\"Dataset successfully extracted.\")\n",
        "\n",
        "# List available commands\n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print(\"Commands:\", commands)\n",
        "\n",
        "# Load dataset using audio_dataset_from_directory\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset=\"both\"\n",
        ")\n",
        "\n",
        "# Check dataset shape\n",
        "for audio, labels in train_ds.take(1):\n",
        "    print(f\"Audio shape: {audio.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "# Convert waveforms into spectrograms\n",
        "def get_spectrogram(waveform):\n",
        "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    return spectrogram\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    return dataset.map(lambda x, y: (get_spectrogram(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_ds = preprocess_dataset(train_ds)\n",
        "val_ds = preprocess_dataset(val_ds)\n",
        "\n",
        "# Build CNN model for speech recognition\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(None, 129)),\n",
        "    layers.Reshape((-1, 129, 1)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(len(commands), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 10\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "\n",
        "# Plot training accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label=\"Train Loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Val Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Function to test the model on a sample audio\n",
        "def predict_sample(index=0):\n",
        "    sample = next(iter(val_ds))\n",
        "    audio_sample = sample[0][index]\n",
        "    label = sample[1][index].numpy()\n",
        "\n",
        "    prediction = model.predict(tf.expand_dims(audio_sample, axis=0))\n",
        "    predicted_label = commands[np.argmax(prediction)]\n",
        "\n",
        "    print(f\"Actual: {commands[label]}\")\n",
        "    print(f\"Predicted: {predicted_label}\")\n",
        "\n",
        "    # Play the audio sample\n",
        "    return Audio(tf.squeeze(audio_sample), rate=16000)\n",
        "\n",
        "# Test the model with a random sample\n",
        "predict_sample()\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"speech_command_model.h5\")\n",
        "\n",
        "# Load the model later if needed\n",
        "# model = tf.keras.models.load_model(\"speech_command_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jCRyYuc7O5f8",
        "outputId": "2be0ea29-c87a-4a33-d05d-d1d4b28f3fff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.7)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (8.1.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.12.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.1.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.67.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Dataset extraction failed. Please check manually.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-957a147ea600>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Confirm dataset extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset extraction failed. Please check manually.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset successfully extracted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset extraction failed. Please check manually."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check dataset storage location\n",
        "!ls -lh ./data/\n",
        "!ls -lh ~/.keras/datasets/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYZxtkhYPSuv",
        "outputId": "72f73011-6fa1-44dd-f683-a4c43bbe46d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 174M\n",
            "drwxr-xr-x 4 root root 4.0K Feb 19 08:03 mini_speech_commands_extracted\n",
            "-rw-r--r-- 1 root root 174M Feb 19 08:03 mini_speech_commands.zip\n",
            "ls: cannot access '/root/.keras/datasets/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Correct dataset path\n",
        "DATASET_PATH = \"data/mini_speech_commands_extracted\"\n",
        "\n",
        "# Verify dataset extraction\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "\n",
        "if not data_dir.exists():\n",
        "    raise FileNotFoundError(f\"Dataset extraction failed. Expected at: {DATASET_PATH}\")\n",
        "else:\n",
        "    print(\"✅ Dataset successfully found!\")\n",
        "\n",
        "# List available commands\n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print(\"Commands:\", commands)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEGV7drCPdYX",
        "outputId": "e1f57c30-0a3e-4d9a-a9b6-a43b9c18d65e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset successfully found!\n",
            "Commands: ['__MACOSX' 'mini_speech_commands']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct dataset path\n",
        "DATASET_PATH = \"data/mini_speech_commands_extracted/mini_speech_commands\"\n",
        "\n",
        "# Verify dataset extraction\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "\n",
        "if not data_dir.exists():\n",
        "    raise FileNotFoundError(f\"Dataset extraction failed. Expected at: {DATASET_PATH}\")\n",
        "else:\n",
        "    print(\"✅ Final dataset path set correctly!\")\n",
        "\n",
        "# List available commands\n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print(\"Commands:\", commands)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fq047nTPs8g",
        "outputId": "f6c4febc-b004-4a2c-c2e8-0001344825a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final dataset path set correctly!\n",
            "Commands: ['right' 'down' 'yes' 'stop' 'no' 'up' 'go' 'left']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxghQXx8PkdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}