{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Locate the zip file\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    'cats_and_dogs_filtered.zip',\n",
        "    origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
        "    extract=False\n",
        ")\n",
        "\n",
        "# Define the dataset extraction path\n",
        "extract_path = os.path.join(os.path.dirname(zip_path), \"cats_and_dogs_filtered\")\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(os.path.dirname(zip_path))\n",
        "\n",
        "# Verify the extracted files\n",
        "dataset_path = extract_path\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"‚úÖ Dataset Extracted Successfully!\")\n",
        "    print(\"üìÇ Dataset Path:\", dataset_path)\n",
        "    print(\"üìÇ Contents:\", os.listdir(dataset_path))  # Should show 'train' and 'validation'\n",
        "else:\n",
        "    print(\"‚ùå Dataset extraction failed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh-DG3siU4z0",
        "outputId": "2642ff34-c4c2-475a-df7e-c222e70ebc90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset Extracted Successfully!\n",
            "üìÇ Dataset Path: /root/.keras/datasets/cats_and_dogs_filtered\n",
            "üìÇ Contents: ['validation', 'vectorize.py', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check where the dataset is stored\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip',\n",
        "                                      origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
        "                                      extract=True)\n",
        "\n",
        "# Get the extracted folder path\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "\n",
        "print(\"Dataset Path:\", PATH)\n",
        "print(\"Contents:\", os.listdir(PATH))  # Check if 'train' and 'validation' exist\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcJxShrVX0g",
        "outputId": "b86d8287-995c-48e9-fc4f-1b5b9d6ba7be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Path: /root/.keras/datasets/cats_and_dogs_filtered\n",
            "Contents: ['validation', 'vectorize.py', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuumCZtwVDvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(PATH, 'cats_and_dogs_filtered/train')\n",
        "val_dir = os.path.join(PATH, 'cats_and_dogs_filtered/validation')\n",
        "\n",
        "# Verify directories exist\n",
        "print(\"Train Directory Exists:\", os.path.exists(train_dir))\n",
        "print(\"Validation Directory Exists:\", os.path.exists(val_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT0vOnmcVd8L",
        "outputId": "d2be926c-3df8-4d90-b449-865715c1fd81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Directory Exists: False\n",
            "Validation Directory Exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhY1fKctVeRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Locate the downloaded dataset\n",
        "dataset_path = tf.keras.utils.get_file(\n",
        "    'cats_and_dogs.zip',\n",
        "    origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "# Get parent directory\n",
        "dataset_dir = os.path.dirname(dataset_path)\n",
        "\n",
        "print(\"Dataset is stored at:\", dataset_dir)\n",
        "print(\"Contents:\", os.listdir(dataset_dir))  # Check extracted files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVS1ocGXVg48",
        "outputId": "5bc73286-bd52-4e25-833d-779b3493f23b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset is stored at: /root/.keras/datasets\n",
            "Contents: ['cats_and_dogs_filtered.zip', 'cats_and_dogs_filtered_extracted', 'cats_and_dogs.zip', 'cats_and_dogs_filtered', 'cats_and_dogs_extracted']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir, 'cats_and_dogs_filtered', 'train')\n",
        "val_dir = os.path.join(dataset_dir, 'cats_and_dogs_filtered', 'validation')\n",
        "\n",
        "# Check if paths exist\n",
        "print(\"Train directory exists:\", os.path.exists(train_dir))\n",
        "print(\"Validation directory exists:\", os.path.exists(val_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8Wib7xvVra0",
        "outputId": "de524dbf-2c60-4127-e174-d28fff0f26d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train directory exists: True\n",
            "Validation directory exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLfbam6KVw5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = os.path.join(dataset_dir, 'cats_and_dogs.zip')\n",
        "extract_path = dataset_dir  # Extract inside .keras/datasets/\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset manually extracted!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU7nETbRV1wk",
        "outputId": "3f5eefe5-0547-42ed-a7f2-aab2449cc15e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset manually extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (160, 160)\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=val_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoXrM_uYV2ED",
        "outputId": "50c41715-6254-49da-afac-fd395405389b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tb3w11woV6QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (160, 160)\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=val_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dva6Y7pVV9PH",
        "outputId": "0e0120e6-68ec-4b8b-fac9-a82d10cd6bfc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p10VsirZV9ha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}